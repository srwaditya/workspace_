# -*- coding: utf-8 -*-
"""Global Energy Consumption (2000-2024).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19D2wUQHxPlmJELFafzUk8NohLoM9D3bG

# Task
create The Global Energy Consumption Dataset provides comprehensive insights into energy usage across different countries and sectors over the past two decades. The dataset aims to help researchers, data analysts, and policymakers understand consumption patterns, identify high-consumption regions, and analyze the impact of renewable energy adoption and carbon emissions.

This dataset allows you to:

Visualize global energy trends.
Predict future consumption patterns.
Analyze the impact of renewable energy and fossil fuels.
Correlate energy usage with economic growth and carbon emissions.
Build machine learning models for energy price forecasting and efficiency analysis. also create plots .

Here is all the data you need:
"global_energy_consumption.csv"

## Data loading

### Subtask:
Load the "global_energy_consumption.csv" dataset.

**Reasoning**:
Load the provided CSV file into a pandas DataFrame.
"""

import pandas as pd

try:
    df = pd.read_csv('global_energy_consumption.csv')
    display(df.head())
except FileNotFoundError:
    print("Error: 'global_energy_consumption.csv' not found.")
    df = None
except pd.errors.ParserError:
    print("Error: Could not parse the CSV file. Please check its format.")
    df = None

"""## Data exploration

### Subtask:
Explore the loaded dataset `df` to understand its characteristics.

**Reasoning**:
I need to explore the dataset by examining its shape, info, descriptive statistics, unique values of categorical columns, and visualize the distribution of key numerical variables.  I will also check for inconsistencies.
"""

# Examine the shape of the DataFrame
print("Shape of the DataFrame:", df.shape)

# Get a summary of the DataFrame
print("\nDataFrame Info:")
df.info()

# Get descriptive statistics for numerical columns
print("\nDescriptive Statistics:")
display(df.describe())

# Explore unique values for categorical columns
print("\nUnique Countries:", df['Country'].unique())
print("\nUnique Years:", df['Year'].unique())

# Visualize the distribution of key numerical variables
import matplotlib.pyplot as plt

plt.figure(figsize=(15, 10))

plt.subplot(2, 2, 1)
plt.hist(df['Total Energy Consumption (TWh)'], bins=20, color='skyblue')
plt.title('Total Energy Consumption (TWh)')

plt.subplot(2, 2, 2)
plt.hist(df['Renewable Energy Share (%)'], bins=20, color='lightgreen')
plt.title('Renewable Energy Share (%)')

plt.subplot(2, 2, 3)
plt.hist(df['Carbon Emissions (Million Tons)'], bins=20, color='salmon')
plt.title('Carbon Emissions (Million Tons)')

plt.subplot(2, 2, 4)
plt.hist(df['Energy Price Index (USD/kWh)'], bins=20, color='plum')
plt.title('Energy Price Index (USD/kWh)')

plt.tight_layout()
plt.show()

# Check for inconsistencies (example: negative energy consumption)
print("\nInconsistencies Check (Negative Energy Consumption):")
negative_energy = df[df['Total Energy Consumption (TWh)'] < 0]
if not negative_energy.empty:
    display(negative_energy)
else:
    print("No negative energy consumption values found.")

"""## Data cleaning

### Subtask:
Clean the dataset by handling potential inconsistencies and data type conversions.

**Reasoning**:
Clean the dataset by handling missing values, checking data types, removing duplicates, and addressing inconsistencies.
"""

# Check for missing values
print("Missing values:\n", df.isnull().sum())

# Check data types and convert if necessary
print("\nData types before conversion:\n", df.dtypes)

# No conversion needed as data types seem correct

# Remove duplicate rows
df.drop_duplicates(inplace=True)
print(f"\nNumber of rows after removing duplicates: {len(df)}")

# Address inconsistencies (e.g., negative energy consumption)
# Check for negative values in relevant columns
negative_energy = df[df['Total Energy Consumption (TWh)'] < 0]
if not negative_energy.empty:
    print("\nNegative energy consumption values found:")
    display(negative_energy)
    # Example: Replace negative values with 0
    df.loc[df['Total Energy Consumption (TWh)'] < 0, 'Total Energy Consumption (TWh)'] = 0
    print("Negative values replaced with 0")

negative_per_capita_energy = df[df['Per Capita Energy Use (kWh)'] < 0]
if not negative_per_capita_energy.empty:
    print("\nNegative Per Capita Energy Use values found:")
    display(negative_per_capita_energy)
    # Example: Replace negative values with 0
    df.loc[df['Per Capita Energy Use (kWh)'] < 0, 'Per Capita Energy Use (kWh)'] = 0
    print("Negative values replaced with 0")
else:
    print("\nNo negative values found in 'Per Capita Energy Use (kWh)' column.")


# Check again for negative energy consumption after handling
negative_energy = df[df['Total Energy Consumption (TWh)'] < 0]
if not negative_energy.empty:
    print("Negative energy consumption values still found:")
    display(negative_energy)
else:
    print("No negative energy consumption values found after handling.")

# Display the cleaned dataframe's info
df.info()

"""## Data wrangling

### Subtask:
Create new features and potentially aggregate data by region or time periods.

**Reasoning**:
Calculate the percentage of renewable energy consumption, aggregate data by country and year, and create additional features.
"""

# Calculate the percentage of renewable energy consumption
df['Renewable Energy Percentage'] = df['Renewable Energy Share (%)']

# Aggregate data by country
df_aggregated = df.groupby('Country').agg({
    'Total Energy Consumption (TWh)': 'mean',
    'Renewable Energy Percentage': 'mean',
    'Carbon Emissions (Million Tons)': 'mean'
}).reset_index()

# Aggregate data by year
df_aggregated_year = df.groupby('Year').agg({
    'Total Energy Consumption (TWh)': 'mean',
    'Renewable Energy Percentage': 'mean',
    'Carbon Emissions (Million Tons)': 'mean'
}).reset_index()

# Create a new feature representing the difference between total energy consumption and renewable energy consumption
df['Energy Consumption Difference'] = df['Total Energy Consumption (TWh)'] * (1 - df['Renewable Energy Percentage'] / 100)

display(df.head())
display(df_aggregated.head())
display(df_aggregated_year.head())

"""## Data analysis

### Subtask:
Analyze correlations between energy consumption, economic growth (if available), and carbon emissions. Identify high-consumption countries/regions and trends in renewable energy adoption.

**Reasoning**:
Calculate the correlation matrix for relevant numerical columns, identify high-consumption countries, and analyze the trend in renewable energy adoption over time.
"""

import matplotlib.pyplot as plt

# Correlation analysis
numerical_cols = ['Total Energy Consumption (TWh)', 'Renewable Energy Share (%)', 'Carbon Emissions (Million Tons)', 'Energy Price Index (USD/kWh)']
if 'Economic Growth Rate (%)' in df.columns:
    numerical_cols.append('Economic Growth Rate (%)')
correlation_matrix = df[numerical_cols].corr()
print("Correlation Matrix:")
display(correlation_matrix)

# High consumption countries
high_consumption_countries = df_aggregated.sort_values(by='Total Energy Consumption (TWh)', ascending=False)
print("\nHigh Consumption Countries:")
display(high_consumption_countries)

# Trend in renewable energy adoption
plt.figure(figsize=(10, 6))
plt.plot(df_aggregated_year['Year'], df_aggregated_year['Renewable Energy Percentage'], marker='o', linestyle='-', color='green')
plt.xlabel('Year')
plt.ylabel('Average Renewable Energy Share (%)')
plt.title('Trend in Renewable Energy Adoption')
plt.grid(True)
plt.show()

"""## Feature engineering

### Subtask:
Prepare the data for machine learning by scaling numerical features and potentially creating interaction terms.

**Reasoning**:
Scale numerical features and create interaction terms as specified in the instructions.
"""

from sklearn.preprocessing import StandardScaler
import pandas as pd
import numpy as np

# 1. Identify numerical features
numerical_features = ['Total Energy Consumption (TWh)', 'Per Capita Energy Use (kWh)', 'Renewable Energy Share (%)',
                      'Fossil Fuel Dependency (%)', 'Industrial Energy Use (%)', 'Household Energy Use (%)',
                      'Carbon Emissions (Million Tons)', 'Energy Price Index (USD/kWh)']


# 2. Choose and instantiate a scaler
scaler = StandardScaler()

# 3. & 4. Fit and transform the numerical features
df[numerical_features] = scaler.fit_transform(df[numerical_features])


# 5. Create interaction terms
df['Renewable_Consumption'] = df['Total Energy Consumption (TWh)'] * df['Renewable Energy Share (%)']
df['Fossil_Consumption'] = df['Total Energy Consumption (TWh)'] * df['Fossil Fuel Dependency (%)']

display(df.head())

"""## Data splitting

### Subtask:
Split the dataset into training, validation, and testing sets.

**Reasoning**:
Split the data into training, validation, and testing sets as instructed, using the specified parameters.
"""

from sklearn.model_selection import train_test_split

# Define features (X) and target (y)
X = df.drop('Total Energy Consumption (TWh)', axis=1)
y = df['Total Energy Consumption (TWh)']

# First split: 80% train, 20% temp (val/test)
X_temp, X_test, y_temp, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=df['Country']
)

# Second split: 50% val, 50% test from temp
X_train, X_val, y_train, y_val = train_test_split(
    X_temp, y_temp, test_size=0.5, random_state=42, stratify=X_temp['Country']
)

"""## Model training

### Subtask:
Train a linear regression model to predict energy consumption.

**Reasoning**:
Train a linear regression model, make predictions on the validation set, and evaluate the model's performance using MAE.
"""

from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error
import numpy as np

# Drop non-numerical features for linear regression
X_train_numeric = X_train.drop(['Country'], axis=1)
X_val_numeric = X_val.drop(['Country'], axis=1)

#Instantiate a linear regression model
model = LinearRegression()

# Train the model
model.fit(X_train_numeric, y_train)

# Make predictions
y_pred = model.predict(X_val_numeric)

# Evaluate the model
mae = mean_absolute_error(y_val, y_pred)
print(f"Mean Absolute Error (MAE): {mae}")

"""## Model optimization

### Subtask:
Optimize the linear regression model by exploring different regularization techniques and hyperparameters.

**Reasoning**:
Import necessary libraries and define parameter grids for Ridge, Lasso, and ElasticNet. Then use GridSearchCV to find the best hyperparameters for each model based on MAE.
"""

from sklearn.linear_model import Ridge, Lasso, ElasticNet
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import mean_absolute_error
import numpy as np

# Drop non-numerical features for linear regression
X_train_numeric = X_train.drop(['Country'], axis=1)
X_val_numeric = X_val.drop(['Country'], axis=1)

# Define parameter grids
alphas = np.logspace(-4, 4, 9)
param_grid_ridge = {'alpha': alphas}
param_grid_lasso = {'alpha': alphas}
param_grid_elasticnet = {'alpha': alphas, 'l1_ratio': [0.1, 0.5, 0.9]}

# Initialize models
ridge = Ridge()
lasso = Lasso()
elasticnet = ElasticNet()

# Perform grid search for Ridge
grid_search_ridge = GridSearchCV(ridge, param_grid_ridge, scoring='neg_mean_absolute_error', cv=5)
grid_search_ridge.fit(X_train_numeric, y_train)

# Perform grid search for Lasso
grid_search_lasso = GridSearchCV(lasso, param_grid_lasso, scoring='neg_mean_absolute_error', cv=5)
grid_search_lasso.fit(X_train_numeric, y_train)

# Perform grid search for ElasticNet
grid_search_elasticnet = GridSearchCV(elasticnet, param_grid_elasticnet, scoring='neg_mean_absolute_error', cv=5)
grid_search_elasticnet.fit(X_train_numeric, y_train)

# Print best hyperparameters and MAE for each model
print("Best hyperparameters and MAE for Ridge:")
print(f"Best alpha: {grid_search_ridge.best_params_['alpha']}")
print(f"Best MAE: {-grid_search_ridge.best_score_}")

print("\nBest hyperparameters and MAE for Lasso:")
print(f"Best alpha: {grid_search_lasso.best_params_['alpha']}")
print(f"Best MAE: {-grid_search_lasso.best_score_}")

print("\nBest hyperparameters and MAE for ElasticNet:")
print(f"Best alpha: {grid_search_elasticnet.best_params_['alpha']}")
print(f"Best l1_ratio: {grid_search_elasticnet.best_params_['l1_ratio']}")
print(f"Best MAE: {-grid_search_elasticnet.best_score_}")

# Determine and store best model based on MAE
best_score = -np.inf
best_model = None
for model_name, grid_search in zip(['Ridge', 'Lasso', 'ElasticNet'], [grid_search_ridge, grid_search_lasso, grid_search_elasticnet]):
    if -grid_search.best_score_ > best_score:
        best_score = -grid_search.best_score_
        best_model = grid_search.best_estimator_
        best_model_name = model_name

print(f"\nBest model: {best_model_name}")

"""## Model evaluation

### Subtask:
Evaluate the best model (identified in the previous step) on the test set.

**Reasoning**:
Evaluate the best model on the test set, calculate the evaluation metrics, and generate a residual plot.
"""

from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import numpy as np
import matplotlib.pyplot as plt

# Prepare test data
X_test_numeric = X_test.drop('Country', axis=1)

# Make predictions
y_pred_test = best_model.predict(X_test_numeric)

# Evaluate the model
rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))
mae = mean_absolute_error(y_test, y_pred_test)
r2 = r2_score(y_test, y_pred_test)

print(f"Test RMSE: {rmse}")
print(f"Test MAE: {mae}")
print(f"Test R-squared: {r2}")

# Residual plot
residuals = y_test - y_pred_test
plt.figure(figsize=(8, 6))
plt.scatter(y_pred_test, residuals, color='blue', alpha=0.5)
plt.axhline(y=0, color='red', linestyle='--')
plt.xlabel("Predicted Values")
plt.ylabel("Residuals")
plt.title("Residual Plot")
plt.show()

"""## Data visualization

### Subtask:
Create visualizations to illustrate key findings from the data analysis and model evaluation.

**Reasoning**:
Create visualizations to illustrate key findings from the data analysis and model evaluation, as per the instructions. This involves creating a line plot of energy consumption trends, visualizations for renewable energy adoption, a heatmap for correlation analysis, plots to visualize model performance, and optional additional visualizations.
"""

import matplotlib.pyplot as plt
import seaborn as sns

# 1. Energy Consumption Trends (Global)
plt.figure(figsize=(10, 6))
plt.plot(df_aggregated_year['Year'], df_aggregated_year['Total Energy Consumption (TWh)'], marker='o', linestyle='-', color='blue')
plt.xlabel('Year')
plt.ylabel('Average Total Energy Consumption (TWh)')
plt.title('Global Energy Consumption Trend')
plt.grid(True)
plt.show()


# 2. Renewable Energy Adoption (Over Time)
plt.figure(figsize=(10, 6))
plt.plot(df_aggregated_year['Year'], df_aggregated_year['Renewable Energy Percentage'], marker='o', linestyle='-', color='green')
plt.xlabel('Year')
plt.ylabel('Average Renewable Energy Share (%)')
plt.title('Trend in Renewable Energy Adoption')
plt.grid(True)
plt.show()

# 3. Correlation Visualization
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Matrix of Energy-Related Variables')
plt.show()

# 4. Model Performance
plt.figure(figsize=(8, 6))
plt.scatter(y_pred_test, y_test, color='blue', alpha=0.5)
plt.xlabel("Predicted Energy Consumption (TWh)")
plt.ylabel("Actual Energy Consumption (TWh)")
plt.title("Predicted vs. Actual Energy Consumption")
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], linestyle='--', color='red')
plt.show()


plt.figure(figsize=(8, 6))
residuals = y_test - y_pred_test
plt.scatter(y_pred_test, residuals, color='blue', alpha=0.5)
plt.axhline(y=0, color='red', linestyle='--')
plt.xlabel("Predicted Values")
plt.ylabel("Residuals")
plt.title("Residual Plot")
plt.show()

"""## Summary:

### 1. Q&A

* **What is the trend in global energy consumption?**  A line plot of `df_aggregated_year` shows the average total energy consumption over time.  (Note: The actual trend cannot be determined without visually inspecting the plot).
* **What is the trend in renewable energy adoption?**  A line plot of `df_aggregated_year` illustrates the average renewable energy share over time. (Note: The actual trend cannot be determined without visually inspecting the plot).
* **What are the correlations between energy consumption, renewable energy share, carbon emissions, and energy price?** A correlation matrix was calculated, revealing the linear relationships between these variables.  The output shows a weak positive correlation between total energy consumption and carbon emissions. Other correlations were weak.
* **Which countries have the highest energy consumption?** The `df_aggregated` DataFrame, sorted by average total energy consumption, identifies the countries with the highest consumption. The USA had the highest average total energy consumption, followed by Japan, India, and other countries.
* **How well does the best model perform on unseen data?** The best model achieved a Test RMSE of 0.00011479298429437922, a Test MAE of 8.358863634009878e-05, and a Test R-squared of 0.9999999872553834.  These metrics suggest a very good fit on the test set.  However, the extremely high R-squared and low error values might indicate overfitting and require further investigation.

### 2. Data Analysis Key Findings

* **Model Performance:** The best performing model (identified through hyperparameter tuning of Ridge, Lasso, and ElasticNet regressions) achieved an extremely low MAE of 8.358863634009878e-05 on the test set, suggesting a very strong fit.  However, such high accuracy could indicate potential overfitting.  The R-squared was 0.9999999872553834.
* **Correlation Analysis:** A weak positive correlation was observed between total energy consumption and carbon emissions.
* **High Consumption Countries:** The USA demonstrated the highest average total energy consumption among the countries analyzed.

### 3. Insights or Next Steps

* **Investigate potential overfitting:** The exceptionally high accuracy of the model warrants further investigation to rule out overfitting. Techniques like cross-validation with more robust metrics or regularization could be explored.  Consider exploring alternative models, such as tree-based models, which might be more robust to outliers or non-linear relationships in the data.
* **Feature Engineering and Selection:** Explore more complex feature engineering techniques (e.g., time-based features, interaction terms, or polynomial features) or feature selection methods (e.g., LASSO or recursive feature elimination) to identify the most predictive features and improve model performance while mitigating overfitting.

"""